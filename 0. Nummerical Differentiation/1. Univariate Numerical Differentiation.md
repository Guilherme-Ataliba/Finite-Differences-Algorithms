This is the study of how to take the derivatives of functions, without calculating the limit. 


#### Order of Error
An error of order $O(x)$ means that to make the algorithm twice more precise, you'll need to make x twice as small (around twice the computational effort).

This idea extends to any kind of order, for example:
- $O(x^2)$ means that to make the algorithm 4 times as precise, you'll need to make x 2 times smaller (around 2 times more computational effort).

# üîµ Univariate Derivative
From calculus, we know that we can **approximate** the derivative at some point $t$ by the slope of the line connecting $f(t)$ and $f(t+\Delta t)$.
![[Pasted image 20240305175419.png|400]]
Given a function $f(t)$ or a **dataset**, the derivative is defined as
$$\frac{df}{dt} = \lim_{\Delta t \to 0} \frac{f(t + \Delta t) - f(t)}{\Delta t}$$
In calculus we take the limit as $\Delta t$ goes to zero, and define that as **the** derivative. Since in computers we can't have infinite precision, to get the best precision possible $\Delta t$ must be as small as possible (that also means more computational effort).

## üî∑ Forward Difference 
Thus, the numerical derivative is defined as the following **finite difference**:
$$\frac{df}{dt} \approx \frac{f(t + \Delta t) - f(t)}{\Delta t}$$
- The smaller the $\Delta t$ (time step), the better the approximation. 


### üîπ Error
To understand the error of our approximation we'll use the Taylor series expansion for $f(t + \Delta t)$ about a base point $f(t)$.
$$f(t+\Delta t) = f(t) + \frac{df(t)}{dt}\Delta t + \frac{d^2f(t)}{df^2}\frac{\Delta t^2}{2!} + \frac{df^3}{dt^3}\frac{\Delta t^3}{3!} + \:...\: + O(\Delta t^4)$$
- $\pmb{O(t)}$: Represent an infinite series, where all of the terms have at least a power of $\Delta t^4$ (or higher)

Now we'll substitute it in the expression for the forward derivative
$$\frac{df}{dt} = {\color{salmon}\frac{df}{dt}} + \frac{d^2f}{dt^2}\frac{\Delta t}{2!} + \frac{d^3f}{dt^3}\frac{\Delta t^2}{3!} + \: ... \: + O(\Delta t^3)$$
- We can see that, the first term is equal to what we want, and the rest is **error**. 

‚ñ∂ We say that the error is of the order of $\Delta t$. 
$$\boxed{\pmb{\text{Error} = O(\Delta t)}}$$
- Since we'll make $\Delta t$ as small as possible, $\Delta t$ will dominate the other terms (if $\Delta t$ is already small, $\Delta t^2$ is negligible).


## üî∑ Backward Difference
Now we'll use the following approximation for the derivative:
![[Pasted image 20240305182135.png|400]]
$$\frac{df}{dt} \approx \frac{f(t) - f(t-\Delta t)}{\Delta t}$$
This is known as the backward difference approximation for the derivative. 
### üîπ Error
Now let's expand $f(t - \Delta t)$ about a base point t:
$$
f(t-\Delta t) = f(t) - \frac{df(t)}{dt}\Delta t + \frac{d^2f(t)}{dt^2}\frac{\Delta t^2}{2!} - \frac{d^2f}{dt^3}\frac{\Delta t^3}{3!} + \:...\: + O(\Delta t^4)
$$
Substituting in the backward difference approximation:
$$\frac{df}{dt} \approx {\color{salmon}\frac{df}{dt}} - \frac{d^2f}{dt^2}\frac{\Delta t}{2!} + \frac{d^3f}{dt^3}\frac{\Delta t^2}{3!} + \: ... \: + O(\Delta t^4)$$
- Just like before, the first term is the exact answer and the rest is error.

The error is then of order of $\Delta t$
$$\boxed{\pmb{Error = O(\Delta t)}}$$
- This means that backward difference and forward difference will have approximately the same error. 


## ‚≠êüî∑ Central Difference
Now, as an approximation for the derivative, we'll use a difference between forward and backward approximation.
- We do this because the errors so far have come from the second term in the Taylor series expansion, so the idea is to cancel that term as well .
![[Pasted image 20240305184036.png|400]]
Now, to better simplify what we want, to further take the difference between the two approximation, we'll use the following derivative approximation
$$\frac{df}{dt} \approx \frac{f(t+\Delta t) - f(t - \Delta t)}{2\Delta t} $$
And for the **error** we'll have the following difference from the Taylor series expansions
$$
f(t+\Delta t) - f(t- \Delta t) = 2\frac{df}{dt} \Delta t + \frac{2\Delta t^3}{3!}\frac{d^3f}{dt^3} + \:...\: + O(\Delta t^5)
$$
Now, substituting:
$$
\frac{df}{dt} \approx {\color{salmon}\frac{df}{dt}} + {\color{skyblue}\frac{\Delta t^2}{3!}} + \:...\: + O(\Delta t^5)
$$
We can see that the error was pushed back to $O(\Delta t^2)$
$$\boxed{\pmb{Error = O(\Delta t^2)}}$$
- This means that, to make the algorithm 100 times as precise, you'll only need to make the $\Delta t$ 10 times smaller. 

#### Why is it better?
While the forward difference overshoots the value of the derivative, and the backwards difference undershoots the value of the derivative, the center difference ends up in the middle, and, in the end, it gets to be a better differentiation scheme. 
![[Pasted image 20240313211240.png]]

### üîπ Higher Order Central Difference
Now we're gonna try to develop a central difference scheme, using more equations, to reduce even more the error arrived from the Taylor series. To do this, we'll use the following 4 equations, expanded in Taylor series:
$$
\displaylines{
f(t + \Delta t) = f(t) + \Delta t f'(t) + \frac{\Delta t^2}{2!}f''(t) + \frac{\Delta t^3}{3!}f'''(t) + ...\\
f(t - \Delta t) = f(t) - \Delta t f'(t) + \frac{\Delta t^2}{2!}f''(t) - \frac{\Delta t^3}{3!}f'''(t) + ...\\
f(t + 2\Delta t) = f(t) + 2\Delta tf'(t) + \frac{4\Delta t^2}{2!}f''(t) + \frac{8\Delta t^3}{3!}f'''(t) + ...\\
f(t - 2\Delta t) = f(t) - 2\Delta t f'(t) + \frac{4\Delta t^2}{2!}f''(t) - \frac{8\Delta t^2}{3!}f'''(t) + ...
}
$$
Subtracting the first two equations we'll get
$$f(t+\Delta t) - f(t- \Delta t) = 2\frac{df}{dt} \Delta t + \frac{2\Delta t^3}{3!}\frac{d^3f}{dt^3} + \:...\: + O(\Delta t^5)$$
Now, subtracting the third and fourth equations
$$f(t + 2\Delta t) - f(t - 2\Delta t) = 4\Delta t f'(t) + \frac{16\Delta t^3}{3!}f'''(t) + ...$$
Now
$$4\big(f(t + \Delta t) - f(t-\Delta t)\big) - \big(f(t+2\Delta t) - f(t - 2\Delta t)\big) = 4\Delta t f'(t) + ...$$
Finally:
$$\boxed{\frac{df}{dt} \approx \frac{4f(t+\Delta t) + f(t-2\Delta t) - 4f(t-\Delta t) - f(t + 2\Delta t)}{4\Delta t}}$$